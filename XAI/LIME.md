# Local Interpretable Model-Agnostic Explanations (LIME)

- Surrogate models -> approximate predictions of black-box models -> trained for single prediction for a single example -> LIME
- Only care about input and output of models that need to be explained -> model-agnostic
- process: generate a new dataset based on initial example + black-box predictions for those samples -> train an interpretable model aka decision tree, linear, ...
- Minimize the difference between the interpretable model's prediction with original prediction
- Has to predefine model complexity aka # of features, also proximity distance (how close for a pertubed sample to influence the local model)

- Advantages:

  - Adaptable to any underlying ML model
  - Flexiable -> text, image, tabular
  - Explanations can use interpretable features instead of original features generated by black-box models

- Disadvantages:
  - Instability -> two closely-related examples can have different explanations
  - Complexity must be defined in advance (how many features ?)
  - Hyperparameters tuning
